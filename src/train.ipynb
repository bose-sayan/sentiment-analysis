{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import transformers\n",
    "from time import time\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "\n",
    "# specify GPU\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Time of Tweet</th>\n",
       "      <th>Age of User</th>\n",
       "      <th>Country</th>\n",
       "      <th>Population -2020</th>\n",
       "      <th>Land Area (Km²)</th>\n",
       "      <th>Density (P/Km²)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "      <td>morning</td>\n",
       "      <td>0-20</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>38928346</td>\n",
       "      <td>652860.0</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "      <td>noon</td>\n",
       "      <td>21-30</td>\n",
       "      <td>Albania</td>\n",
       "      <td>2877797</td>\n",
       "      <td>27400.0</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "      <td>night</td>\n",
       "      <td>31-45</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>43851044</td>\n",
       "      <td>2381740.0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "      <td>morning</td>\n",
       "      <td>46-60</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>77265</td>\n",
       "      <td>470.0</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "      <td>noon</td>\n",
       "      <td>60-70</td>\n",
       "      <td>Angola</td>\n",
       "      <td>32866272</td>\n",
       "      <td>1246700.0</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "\n",
       "                         selected_text sentiment Time of Tweet Age of User  \\\n",
       "0  I`d have responded, if I were going   neutral       morning        0-20   \n",
       "1                             Sooo SAD  negative          noon       21-30   \n",
       "2                          bullying me  negative         night       31-45   \n",
       "3                       leave me alone  negative       morning       46-60   \n",
       "4                        Sons of ****,  negative          noon       60-70   \n",
       "\n",
       "       Country  Population -2020  Land Area (Km²)  Density (P/Km²)  \n",
       "0  Afghanistan          38928346         652860.0               60  \n",
       "1      Albania           2877797          27400.0              105  \n",
       "2      Algeria          43851044        2381740.0               18  \n",
       "3      Andorra             77265            470.0              164  \n",
       "4       Angola          32866272        1246700.0               26  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/train.csv\", encoding=\"unicode_escape\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "textID              0\n",
       "text                1\n",
       "selected_text       1\n",
       "sentiment           0\n",
       "Time of Tweet       0\n",
       "Age of User         0\n",
       "Country             0\n",
       "Population -2020    0\n",
       "Land Area (Km²)     0\n",
       "Density (P/Km²)     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for NaN values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Time of Tweet</th>\n",
       "      <th>Age of User</th>\n",
       "      <th>Country</th>\n",
       "      <th>Population -2020</th>\n",
       "      <th>Land Area (Km²)</th>\n",
       "      <th>Density (P/Km²)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "      <td>morning</td>\n",
       "      <td>0-20</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>38928346</td>\n",
       "      <td>652860.0</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "      <td>noon</td>\n",
       "      <td>21-30</td>\n",
       "      <td>Albania</td>\n",
       "      <td>2877797</td>\n",
       "      <td>27400.0</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "      <td>night</td>\n",
       "      <td>31-45</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>43851044</td>\n",
       "      <td>2381740.0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "      <td>morning</td>\n",
       "      <td>46-60</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>77265</td>\n",
       "      <td>470.0</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "      <td>noon</td>\n",
       "      <td>60-70</td>\n",
       "      <td>Angola</td>\n",
       "      <td>32866272</td>\n",
       "      <td>1246700.0</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "\n",
       "                         selected_text sentiment Time of Tweet Age of User  \\\n",
       "0  I`d have responded, if I were going   neutral       morning        0-20   \n",
       "1                             Sooo SAD  negative          noon       21-30   \n",
       "2                          bullying me  negative         night       31-45   \n",
       "3                       leave me alone  negative       morning       46-60   \n",
       "4                        Sons of ****,  negative          noon       60-70   \n",
       "\n",
       "       Country  Population -2020  Land Area (Km²)  Density (P/Km²)  \n",
       "0  Afghanistan          38928346         652860.0               60  \n",
       "1      Albania           2877797          27400.0              105  \n",
       "2      Algeria          43851044        2381740.0               18  \n",
       "3      Andorra             77265            470.0              164  \n",
       "4       Angola          32866272        1246700.0               26  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove NaN values\n",
    "df = df.dropna()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27480, 10)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "neutral     0.404549\n",
       "positive    0.312300\n",
       "negative    0.283151\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check class distribution\n",
    "df[\"sentiment\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train dataset into train, validation\n",
    "train_text, val_text, train_label, val_label = train_test_split(\n",
    "    df[\"selected_text\"],\n",
    "    df[\"sentiment\"],\n",
    "    random_state=42,\n",
    "    test_size=0.3,\n",
    "    stratify=df[\"sentiment\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import BERT-base pretrained model\n",
    "bert = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get length of all the messages in the train set\n",
    "seq_len = [len(str(i).split()) for i in train_text]\n",
    "pd.Series(seq_len).hist(bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_list = train_text.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and encode sequences in the training set\n",
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    train_text.tolist(),\n",
    "    max_length=max_seq_len,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False,\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the validation set\n",
    "tokens_val = tokenizer.batch_encode_plus(\n",
    "    val_text.tolist(),\n",
    "    max_length=max_seq_len,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for train set\n",
    "train_seq = torch.tensor(tokens_train[\"input_ids\"])\n",
    "train_mask = torch.tensor(tokens_train[\"attention_mask\"])\n",
    "sentiments = {\"neutral\": 0, \"positive\": 1, \"negative\": 2}\n",
    "train_y = torch.tensor([sentiments[i] for i in train_label.tolist()])\n",
    "\n",
    "# for validation set\n",
    "val_seq = torch.tensor(tokens_val[\"input_ids\"])\n",
    "val_mask = torch.tensor(tokens_val[\"attention_mask\"])\n",
    "val_y = torch.tensor([sentiments[i] for i in val_label.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# define a batch size\n",
    "batch_size = 32\n",
    "\n",
    "# wrap tensors\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "# sampler for sampling the data during training\n",
    "train_sampler = RandomSampler(train_data)\n",
    "# dataLoader for train set\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# wrap tensors\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "# sampler for sampling the data during training\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "# dataLoader for validation set\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze BERT Parameters\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model Architecture\n",
    "class BERT_Arch(nn.Module):\n",
    "    def __init__(self, bert):\n",
    "        super(BERT_Arch, self).__init__()\n",
    "        self.bert = bert\n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        # relu activation function\n",
    "        self.relu = nn.ReLU()\n",
    "        # dense layer 1\n",
    "        self.fc1 = nn.Linear(768, 512)\n",
    "        # dense layer 2 (Output layer)\n",
    "        self.fc2 = nn.Linear(512, 3)\n",
    "        # softmax activation function\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    # define the forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "        # pass the inputs to the model\n",
    "        _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n",
    "        x = self.fc1(cls_hs)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        # output layer\n",
    "        x = self.fc2(x)\n",
    "        # apply softmax activation\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the pre-trained BERT to our define architecture\n",
    "model = BERT_Arch(bert)\n",
    "\n",
    "# push the model to GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer from hugging face transformers\n",
    "from transformers import AdamW\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.17716174 0.82395271 1.06742134]\n"
     ]
    }
   ],
   "source": [
    "# Find Class Weights\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# compute the class weights\n",
    "class_wts = compute_class_weight(\n",
    "    \"balanced\", classes=np.unique(train_label), y=train_label\n",
    ")\n",
    "\n",
    "print(class_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class weights to tensor\n",
    "weights = torch.tensor(class_wts, dtype=torch.float)\n",
    "weights = weights.to(device)\n",
    "\n",
    "# loss function\n",
    "cross_entropy = nn.NLLLoss(weight=weights)\n",
    "\n",
    "# number of training epochs\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine Tune BERT\n",
    "# function to train the model\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    # empty list to save model predictions\n",
    "    total_preds = []\n",
    "    # iterate over batches\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # progress update after every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            print(\"  Batch {:>5,}  of  {:>5,}.\".format(step, len(train_dataloader)))\n",
    "        # push the batch to gpu\n",
    "        batch = [r.to(device) for r in batch]\n",
    "        sent_id, mask, labels = batch\n",
    "        # clear previously calculated gradients\n",
    "        model.zero_grad()\n",
    "        # get model predictions for the current batch\n",
    "        preds = model(sent_id, mask)\n",
    "        # compute the loss between actual and predicted values\n",
    "        loss = cross_entropy(preds, labels)\n",
    "        # add on to the total loss\n",
    "        total_loss = total_loss + loss.item()\n",
    "        # backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        # model predictions are stored on GPU. So, push it to CPU\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "        # append the model predictions\n",
    "        total_preds.append(preds)\n",
    "    # compute the training loss of the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds = np.concatenate(total_preds, axis=0)\n",
    "    # returns the loss and predictions\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for evaluating the model\n",
    "def evaluate():\n",
    "    print(\"\\nEvaluating...\")\n",
    "    # deactivate dropout layers\n",
    "    model.eval()\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    # empty list to save the model predictions\n",
    "    total_preds = []\n",
    "    # iterate over batches\n",
    "    for step, batch in enumerate(val_dataloader):\n",
    "        # Progress update every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            # Report progress.\n",
    "            print(\"  Batch {:>5,}  of  {:>5,}.\".format(step, len(val_dataloader)))\n",
    "        # push the batch to gpu\n",
    "        batch = [t.to(device) for t in batch]\n",
    "        sent_id, mask, labels = batch\n",
    "        # deactivate autograd\n",
    "        with torch.no_grad():\n",
    "            # model predictions\n",
    "            preds = model(sent_id, mask)\n",
    "            # compute the validation loss between actual and predicted values\n",
    "            loss = cross_entropy(preds, labels)\n",
    "            total_loss = total_loss + loss.item()\n",
    "            preds = preds.detach().cpu().numpy()\n",
    "            total_preds.append(preds)\n",
    "    # compute the validation loss of the epoch\n",
    "    avg_loss = total_loss / len(val_dataloader)\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds = np.concatenate(total_preds, axis=0)\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 30\n",
      "  Batch    50  of    602.\n",
      "  Batch   100  of    602.\n",
      "  Batch   150  of    602.\n",
      "  Batch   200  of    602.\n",
      "  Batch   250  of    602.\n",
      "  Batch   300  of    602.\n",
      "  Batch   350  of    602.\n",
      "  Batch   400  of    602.\n",
      "  Batch   450  of    602.\n",
      "  Batch   500  of    602.\n",
      "  Batch   550  of    602.\n",
      "  Batch   600  of    602.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    258.\n",
      "  Batch   100  of    258.\n",
      "  Batch   150  of    258.\n",
      "  Batch   200  of    258.\n",
      "  Batch   250  of    258.\n",
      "\n",
      "Training Loss: 0.789\n",
      "Validation Loss: 0.637\n",
      "\n",
      " Epoch 2 / 30\n",
      "  Batch    50  of    602.\n",
      "  Batch   100  of    602.\n",
      "  Batch   150  of    602.\n",
      "  Batch   200  of    602.\n",
      "  Batch   250  of    602.\n",
      "  Batch   300  of    602.\n",
      "  Batch   350  of    602.\n",
      "  Batch   400  of    602.\n",
      "  Batch   450  of    602.\n",
      "  Batch   500  of    602.\n",
      "  Batch   550  of    602.\n",
      "  Batch   600  of    602.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    258.\n",
      "  Batch   100  of    258.\n",
      "  Batch   150  of    258.\n",
      "  Batch   200  of    258.\n",
      "  Batch   250  of    258.\n",
      "\n",
      "Training Loss: 0.648\n",
      "Validation Loss: 0.568\n",
      "\n",
      " Epoch 3 / 30\n",
      "  Batch    50  of    602.\n",
      "  Batch   100  of    602.\n",
      "  Batch   150  of    602.\n",
      "  Batch   200  of    602.\n",
      "  Batch   250  of    602.\n",
      "  Batch   300  of    602.\n",
      "  Batch   350  of    602.\n",
      "  Batch   400  of    602.\n",
      "  Batch   450  of    602.\n",
      "  Batch   500  of    602.\n",
      "  Batch   550  of    602.\n",
      "  Batch   600  of    602.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    258.\n",
      "  Batch   100  of    258.\n",
      "  Batch   150  of    258.\n",
      "  Batch   200  of    258.\n",
      "  Batch   250  of    258.\n",
      "\n",
      "Training Loss: 0.613\n",
      "Validation Loss: 0.593\n",
      "\n",
      " Epoch 4 / 30\n",
      "  Batch    50  of    602.\n",
      "  Batch   100  of    602.\n",
      "  Batch   150  of    602.\n",
      "  Batch   200  of    602.\n",
      "  Batch   250  of    602.\n",
      "  Batch   300  of    602.\n",
      "  Batch   350  of    602.\n",
      "  Batch   400  of    602.\n",
      "  Batch   450  of    602.\n",
      "  Batch   500  of    602.\n",
      "  Batch   550  of    602.\n",
      "  Batch   600  of    602.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    258.\n",
      "  Batch   100  of    258.\n",
      "  Batch   150  of    258.\n",
      "  Batch   200  of    258.\n",
      "  Batch   250  of    258.\n",
      "\n",
      "Training Loss: 0.606\n",
      "Validation Loss: 0.641\n",
      "\n",
      " Epoch 5 / 30\n",
      "  Batch    50  of    602.\n",
      "  Batch   100  of    602.\n",
      "  Batch   150  of    602.\n",
      "  Batch   200  of    602.\n",
      "  Batch   250  of    602.\n",
      "  Batch   300  of    602.\n",
      "  Batch   350  of    602.\n",
      "  Batch   400  of    602.\n",
      "  Batch   450  of    602.\n",
      "  Batch   500  of    602.\n",
      "  Batch   550  of    602.\n",
      "  Batch   600  of    602.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    258.\n",
      "  Batch   100  of    258.\n",
      "  Batch   150  of    258.\n",
      "  Batch   200  of    258.\n",
      "  Batch   250  of    258.\n",
      "\n",
      "Training Loss: 0.591\n",
      "Validation Loss: 0.686\n",
      "\n",
      " Epoch 6 / 30\n",
      "  Batch    50  of    602.\n",
      "  Batch   100  of    602.\n",
      "  Batch   150  of    602.\n",
      "  Batch   200  of    602.\n",
      "  Batch   250  of    602.\n",
      "  Batch   300  of    602.\n",
      "  Batch   350  of    602.\n",
      "  Batch   400  of    602.\n",
      "  Batch   450  of    602.\n",
      "  Batch   500  of    602.\n",
      "  Batch   550  of    602.\n",
      "  Batch   600  of    602.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    258.\n",
      "  Batch   100  of    258.\n",
      "  Batch   150  of    258.\n",
      "  Batch   200  of    258.\n",
      "  Batch   250  of    258.\n",
      "\n",
      "Training Loss: 0.582\n",
      "Validation Loss: 0.512\n",
      "\n",
      " Epoch 7 / 30\n",
      "  Batch    50  of    602.\n",
      "  Batch   100  of    602.\n",
      "  Batch   150  of    602.\n",
      "  Batch   200  of    602.\n",
      "  Batch   250  of    602.\n",
      "  Batch   300  of    602.\n",
      "  Batch   350  of    602.\n",
      "  Batch   400  of    602.\n",
      "  Batch   450  of    602.\n",
      "  Batch   500  of    602.\n",
      "  Batch   550  of    602.\n",
      "  Batch   600  of    602.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    258.\n",
      "  Batch   100  of    258.\n",
      "  Batch   150  of    258.\n",
      "  Batch   200  of    258.\n",
      "  Batch   250  of    258.\n",
      "\n",
      "Training Loss: 0.571\n",
      "Validation Loss: 0.528\n",
      "\n",
      " Epoch 8 / 30\n",
      "  Batch    50  of    602.\n",
      "  Batch   100  of    602.\n",
      "  Batch   150  of    602.\n",
      "  Batch   200  of    602.\n",
      "  Batch   250  of    602.\n",
      "  Batch   300  of    602.\n",
      "  Batch   350  of    602.\n",
      "  Batch   400  of    602.\n",
      "  Batch   450  of    602.\n",
      "  Batch   500  of    602.\n",
      "  Batch   550  of    602.\n",
      "  Batch   600  of    602.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    258.\n",
      "  Batch   100  of    258.\n",
      "  Batch   150  of    258.\n",
      "  Batch   200  of    258.\n",
      "  Batch   250  of    258.\n",
      "\n",
      "Training Loss: 0.572\n",
      "Validation Loss: 0.533\n",
      "\n",
      " Epoch 9 / 30\n",
      "  Batch    50  of    602.\n",
      "  Batch   100  of    602.\n",
      "  Batch   150  of    602.\n",
      "  Batch   200  of    602.\n",
      "  Batch   250  of    602.\n",
      "  Batch   300  of    602.\n",
      "  Batch   350  of    602.\n",
      "  Batch   400  of    602.\n",
      "  Batch   450  of    602.\n",
      "  Batch   500  of    602.\n",
      "  Batch   550  of    602.\n",
      "  Batch   600  of    602.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    258.\n",
      "  Batch   100  of    258.\n",
      "  Batch   150  of    258.\n",
      "  Batch   200  of    258.\n",
      "  Batch   250  of    258.\n",
      "\n",
      "Training Loss: 0.565\n",
      "Validation Loss: 0.500\n",
      "\n",
      " Epoch 10 / 30\n",
      "  Batch    50  of    602.\n",
      "  Batch   100  of    602.\n",
      "  Batch   150  of    602.\n",
      "  Batch   200  of    602.\n",
      "  Batch   250  of    602.\n",
      "  Batch   300  of    602.\n",
      "  Batch   350  of    602.\n",
      "  Batch   400  of    602.\n",
      "  Batch   450  of    602.\n",
      "  Batch   500  of    602.\n",
      "  Batch   550  of    602.\n",
      "  Batch   600  of    602.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    258.\n",
      "  Batch   100  of    258.\n",
      "  Batch   150  of    258.\n",
      "  Batch   200  of    258.\n",
      "  Batch   250  of    258.\n",
      "\n",
      "Training Loss: 0.563\n",
      "Validation Loss: 0.492\n",
      "\n",
      " Epoch 11 / 30\n",
      "  Batch    50  of    602.\n",
      "  Batch   100  of    602.\n",
      "  Batch   150  of    602.\n",
      "  Batch   200  of    602.\n",
      "  Batch   250  of    602.\n",
      "  Batch   300  of    602.\n",
      "  Batch   350  of    602.\n",
      "  Batch   400  of    602.\n",
      "  Batch   450  of    602.\n",
      "  Batch   500  of    602.\n",
      "  Batch   550  of    602.\n",
      "  Batch   600  of    602.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    258.\n",
      "  Batch   100  of    258.\n",
      "  Batch   150  of    258.\n",
      "  Batch   200  of    258.\n",
      "  Batch   250  of    258.\n",
      "\n",
      "Training Loss: 0.555\n",
      "Validation Loss: 0.542\n",
      "\n",
      " Epoch 12 / 30\n",
      "  Batch    50  of    602.\n",
      "  Batch   100  of    602.\n",
      "  Batch   150  of    602.\n",
      "  Batch   200  of    602.\n",
      "  Batch   250  of    602.\n",
      "  Batch   300  of    602.\n",
      "  Batch   350  of    602.\n",
      "  Batch   400  of    602.\n",
      "  Batch   450  of    602.\n",
      "  Batch   500  of    602.\n",
      "  Batch   550  of    602.\n",
      "  Batch   600  of    602.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    258.\n",
      "  Batch   100  of    258.\n",
      "  Batch   150  of    258.\n",
      "  Batch   200  of    258.\n",
      "  Batch   250  of    258.\n",
      "\n",
      "Training Loss: 0.552\n",
      "Validation Loss: 0.492\n",
      "\n",
      " Epoch 13 / 30\n",
      "  Batch    50  of    602.\n",
      "  Batch   100  of    602.\n",
      "  Batch   150  of    602.\n",
      "  Batch   200  of    602.\n",
      "  Batch   250  of    602.\n",
      "  Batch   300  of    602.\n",
      "  Batch   350  of    602.\n",
      "  Batch   400  of    602.\n",
      "  Batch   450  of    602.\n",
      "  Batch   500  of    602.\n",
      "  Batch   550  of    602.\n",
      "  Batch   600  of    602.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    258.\n",
      "  Batch   100  of    258.\n",
      "  Batch   150  of    258.\n",
      "  Batch   200  of    258.\n",
      "  Batch   250  of    258.\n",
      "\n",
      "Training Loss: 0.549\n",
      "Validation Loss: 0.515\n",
      "\n",
      " Epoch 14 / 30\n",
      "  Batch    50  of    602.\n",
      "  Batch   100  of    602.\n",
      "  Batch   150  of    602.\n",
      "  Batch   200  of    602.\n",
      "  Batch   250  of    602.\n",
      "  Batch   300  of    602.\n",
      "  Batch   350  of    602.\n",
      "  Batch   400  of    602.\n",
      "  Batch   450  of    602.\n",
      "  Batch   500  of    602.\n",
      "  Batch   550  of    602.\n",
      "  Batch   600  of    602.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    258.\n",
      "  Batch   100  of    258.\n",
      "  Batch   150  of    258.\n",
      "  Batch   200  of    258.\n",
      "  Batch   250  of    258.\n",
      "\n",
      "Training Loss: 0.543\n",
      "Validation Loss: 0.490\n",
      "\n",
      " Epoch 15 / 30\n",
      "  Batch    50  of    602.\n",
      "  Batch   100  of    602.\n",
      "  Batch   150  of    602.\n",
      "  Batch   200  of    602.\n",
      "  Batch   250  of    602.\n",
      "  Batch   300  of    602.\n",
      "  Batch   350  of    602.\n",
      "  Batch   400  of    602.\n",
      "  Batch   450  of    602.\n",
      "  Batch   500  of    602.\n",
      "  Batch   550  of    602.\n",
      "  Batch   600  of    602.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    258.\n",
      "  Batch   100  of    258.\n",
      "  Batch   150  of    258.\n",
      "  Batch   200  of    258.\n",
      "  Batch   250  of    258.\n",
      "\n",
      "Training Loss: 0.545\n",
      "Validation Loss: 0.521\n",
      "\n",
      " Epoch 16 / 30\n",
      "  Batch    50  of    602.\n",
      "  Batch   100  of    602.\n",
      "  Batch   150  of    602.\n",
      "  Batch   200  of    602.\n",
      "  Batch   250  of    602.\n",
      "  Batch   300  of    602.\n",
      "  Batch   350  of    602.\n",
      "  Batch   400  of    602.\n",
      "  Batch   450  of    602.\n",
      "  Batch   500  of    602.\n",
      "  Batch   550  of    602.\n",
      "  Batch   600  of    602.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    258.\n",
      "  Batch   100  of    258.\n",
      "  Batch   150  of    258.\n",
      "  Batch   200  of    258.\n",
      "  Batch   250  of    258.\n",
      "\n",
      "Training Loss: 0.544\n",
      "Validation Loss: 0.500\n",
      "\n",
      " Epoch 17 / 30\n",
      "  Batch    50  of    602.\n",
      "  Batch   100  of    602.\n",
      "  Batch   150  of    602.\n",
      "  Batch   200  of    602.\n",
      "  Batch   250  of    602.\n",
      "  Batch   300  of    602.\n",
      "  Batch   350  of    602.\n",
      "  Batch   400  of    602.\n",
      "  Batch   450  of    602.\n",
      "  Batch   500  of    602.\n",
      "  Batch   550  of    602.\n",
      "  Batch   600  of    602.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    258.\n",
      "  Batch   100  of    258.\n",
      "  Batch   150  of    258.\n",
      "  Batch   200  of    258.\n",
      "  Batch   250  of    258.\n",
      "\n",
      "Training Loss: 0.541\n",
      "Validation Loss: 0.574\n",
      "\n",
      " Epoch 18 / 30\n",
      "  Batch    50  of    602.\n",
      "  Batch   100  of    602.\n",
      "  Batch   150  of    602.\n",
      "  Batch   200  of    602.\n",
      "  Batch   250  of    602.\n",
      "  Batch   300  of    602.\n",
      "  Batch   350  of    602.\n",
      "  Batch   400  of    602.\n",
      "  Batch   450  of    602.\n",
      "  Batch   500  of    602.\n",
      "  Batch   550  of    602.\n",
      "  Batch   600  of    602.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    258.\n",
      "  Batch   100  of    258.\n",
      "  Batch   150  of    258.\n",
      "  Batch   200  of    258.\n",
      "  Batch   250  of    258.\n",
      "\n",
      "Training Loss: 0.539\n",
      "Validation Loss: 0.483\n",
      "\n",
      " Epoch 19 / 30\n",
      "  Batch    50  of    602.\n",
      "  Batch   100  of    602.\n",
      "  Batch   150  of    602.\n",
      "  Batch   200  of    602.\n",
      "  Batch   250  of    602.\n",
      "  Batch   300  of    602.\n",
      "  Batch   350  of    602.\n",
      "  Batch   400  of    602.\n",
      "  Batch   450  of    602.\n",
      "  Batch   500  of    602.\n",
      "  Batch   550  of    602.\n",
      "  Batch   600  of    602.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    258.\n",
      "  Batch   100  of    258.\n",
      "  Batch   150  of    258.\n",
      "  Batch   200  of    258.\n",
      "  Batch   250  of    258.\n",
      "\n",
      "Training Loss: 0.538\n",
      "Validation Loss: 0.559\n",
      "\n",
      " Epoch 20 / 30\n",
      "  Batch    50  of    602.\n",
      "  Batch   100  of    602.\n",
      "  Batch   150  of    602.\n",
      "  Batch   200  of    602.\n",
      "  Batch   250  of    602.\n",
      "  Batch   300  of    602.\n",
      "  Batch   350  of    602.\n",
      "  Batch   400  of    602.\n",
      "  Batch   450  of    602.\n",
      "  Batch   500  of    602.\n",
      "  Batch   550  of    602.\n",
      "  Batch   600  of    602.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    258.\n",
      "  Batch   100  of    258.\n",
      "  Batch   150  of    258.\n",
      "  Batch   200  of    258.\n",
      "  Batch   250  of    258.\n",
      "\n",
      "Training Loss: 0.534\n",
      "Validation Loss: 0.489\n",
      "\n",
      " Epoch 21 / 30\n",
      "  Batch    50  of    602.\n",
      "  Batch   100  of    602.\n",
      "  Batch   150  of    602.\n",
      "  Batch   200  of    602.\n",
      "  Batch   250  of    602.\n",
      "  Batch   300  of    602.\n",
      "  Batch   350  of    602.\n",
      "  Batch   400  of    602.\n",
      "  Batch   450  of    602.\n",
      "  Batch   500  of    602.\n",
      "  Batch   550  of    602.\n",
      "  Batch   600  of    602.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    258.\n",
      "  Batch   100  of    258.\n",
      "  Batch   150  of    258.\n",
      "  Batch   200  of    258.\n",
      "  Batch   250  of    258.\n",
      "\n",
      "Training Loss: 0.535\n",
      "Validation Loss: 0.499\n",
      "\n",
      " Epoch 22 / 30\n",
      "  Batch    50  of    602.\n",
      "  Batch   100  of    602.\n",
      "  Batch   150  of    602.\n",
      "  Batch   200  of    602.\n",
      "  Batch   250  of    602.\n",
      "  Batch   300  of    602.\n",
      "  Batch   350  of    602.\n",
      "  Batch   400  of    602.\n",
      "  Batch   450  of    602.\n",
      "  Batch   500  of    602.\n",
      "  Batch   550  of    602.\n",
      "  Batch   600  of    602.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    258.\n",
      "  Batch   100  of    258.\n",
      "  Batch   150  of    258.\n",
      "  Batch   200  of    258.\n",
      "  Batch   250  of    258.\n",
      "\n",
      "Training Loss: 0.533\n",
      "Validation Loss: 0.472\n",
      "\n",
      " Epoch 23 / 30\n",
      "  Batch    50  of    602.\n",
      "  Batch   100  of    602.\n",
      "  Batch   150  of    602.\n",
      "  Batch   200  of    602.\n",
      "  Batch   250  of    602.\n",
      "  Batch   300  of    602.\n",
      "  Batch   350  of    602.\n",
      "  Batch   400  of    602.\n",
      "  Batch   450  of    602.\n",
      "  Batch   500  of    602.\n",
      "  Batch   550  of    602.\n",
      "  Batch   600  of    602.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    258.\n",
      "  Batch   100  of    258.\n",
      "  Batch   150  of    258.\n",
      "  Batch   200  of    258.\n",
      "  Batch   250  of    258.\n",
      "\n",
      "Training Loss: 0.535\n",
      "Validation Loss: 0.468\n",
      "\n",
      " Epoch 24 / 30\n",
      "  Batch    50  of    602.\n",
      "  Batch   100  of    602.\n",
      "  Batch   150  of    602.\n",
      "  Batch   200  of    602.\n",
      "  Batch   250  of    602.\n",
      "  Batch   300  of    602.\n",
      "  Batch   350  of    602.\n",
      "  Batch   400  of    602.\n",
      "  Batch   450  of    602.\n",
      "  Batch   500  of    602.\n",
      "  Batch   550  of    602.\n",
      "  Batch   600  of    602.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    258.\n",
      "  Batch   100  of    258.\n",
      "  Batch   150  of    258.\n",
      "  Batch   200  of    258.\n",
      "  Batch   250  of    258.\n",
      "\n",
      "Training Loss: 0.536\n",
      "Validation Loss: 0.493\n",
      "\n",
      " Epoch 25 / 30\n",
      "  Batch    50  of    602.\n",
      "  Batch   100  of    602.\n",
      "  Batch   150  of    602.\n",
      "  Batch   200  of    602.\n",
      "  Batch   250  of    602.\n",
      "  Batch   300  of    602.\n",
      "  Batch   350  of    602.\n",
      "  Batch   400  of    602.\n",
      "  Batch   450  of    602.\n",
      "  Batch   500  of    602.\n",
      "  Batch   550  of    602.\n",
      "  Batch   600  of    602.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    258.\n",
      "  Batch   100  of    258.\n",
      "  Batch   150  of    258.\n",
      "  Batch   200  of    258.\n",
      "  Batch   250  of    258.\n",
      "\n",
      "Training Loss: 0.524\n",
      "Validation Loss: 0.469\n",
      "\n",
      " Epoch 26 / 30\n",
      "  Batch    50  of    602.\n",
      "  Batch   100  of    602.\n",
      "  Batch   150  of    602.\n",
      "  Batch   200  of    602.\n",
      "  Batch   250  of    602.\n",
      "  Batch   300  of    602.\n",
      "  Batch   350  of    602.\n",
      "  Batch   400  of    602.\n",
      "  Batch   450  of    602.\n",
      "  Batch   500  of    602.\n",
      "  Batch   550  of    602.\n",
      "  Batch   600  of    602.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    258.\n",
      "  Batch   100  of    258.\n",
      "  Batch   150  of    258.\n",
      "  Batch   200  of    258.\n",
      "  Batch   250  of    258.\n",
      "\n",
      "Training Loss: 0.527\n",
      "Validation Loss: 0.497\n",
      "\n",
      " Epoch 27 / 30\n",
      "  Batch    50  of    602.\n",
      "  Batch   100  of    602.\n",
      "  Batch   150  of    602.\n",
      "  Batch   200  of    602.\n",
      "  Batch   250  of    602.\n",
      "  Batch   300  of    602.\n",
      "  Batch   350  of    602.\n",
      "  Batch   400  of    602.\n",
      "  Batch   450  of    602.\n",
      "  Batch   500  of    602.\n",
      "  Batch   550  of    602.\n",
      "  Batch   600  of    602.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    258.\n",
      "  Batch   100  of    258.\n",
      "  Batch   150  of    258.\n",
      "  Batch   200  of    258.\n",
      "  Batch   250  of    258.\n",
      "\n",
      "Training Loss: 0.527\n",
      "Validation Loss: 0.499\n",
      "\n",
      " Epoch 28 / 30\n",
      "  Batch    50  of    602.\n",
      "  Batch   100  of    602.\n",
      "  Batch   150  of    602.\n",
      "  Batch   200  of    602.\n",
      "  Batch   250  of    602.\n",
      "  Batch   300  of    602.\n",
      "  Batch   350  of    602.\n",
      "  Batch   400  of    602.\n",
      "  Batch   450  of    602.\n",
      "  Batch   500  of    602.\n",
      "  Batch   550  of    602.\n",
      "  Batch   600  of    602.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    258.\n",
      "  Batch   100  of    258.\n",
      "  Batch   150  of    258.\n",
      "  Batch   200  of    258.\n",
      "  Batch   250  of    258.\n",
      "\n",
      "Training Loss: 0.528\n",
      "Validation Loss: 0.486\n",
      "\n",
      " Epoch 29 / 30\n",
      "  Batch    50  of    602.\n",
      "  Batch   100  of    602.\n",
      "  Batch   150  of    602.\n",
      "  Batch   200  of    602.\n",
      "  Batch   250  of    602.\n",
      "  Batch   300  of    602.\n",
      "  Batch   350  of    602.\n",
      "  Batch   400  of    602.\n",
      "  Batch   450  of    602.\n",
      "  Batch   500  of    602.\n",
      "  Batch   550  of    602.\n",
      "  Batch   600  of    602.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    258.\n",
      "  Batch   100  of    258.\n",
      "  Batch   150  of    258.\n",
      "  Batch   200  of    258.\n",
      "  Batch   250  of    258.\n",
      "\n",
      "Training Loss: 0.531\n",
      "Validation Loss: 0.479\n",
      "\n",
      " Epoch 30 / 30\n",
      "  Batch    50  of    602.\n",
      "  Batch   100  of    602.\n",
      "  Batch   150  of    602.\n",
      "  Batch   200  of    602.\n",
      "  Batch   250  of    602.\n",
      "  Batch   300  of    602.\n",
      "  Batch   350  of    602.\n",
      "  Batch   400  of    602.\n",
      "  Batch   450  of    602.\n",
      "  Batch   500  of    602.\n",
      "  Batch   550  of    602.\n",
      "  Batch   600  of    602.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    258.\n",
      "  Batch   100  of    258.\n",
      "  Batch   150  of    258.\n",
      "  Batch   200  of    258.\n",
      "  Batch   250  of    258.\n",
      "\n",
      "Training Loss: 0.523\n",
      "Validation Loss: 0.506\n"
     ]
    }
   ],
   "source": [
    "# Start Model Training\n",
    "# set initial loss to infinite\n",
    "best_valid_loss = float(\"inf\")\n",
    "\n",
    "# empty lists to store training and validation loss of each epoch\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "# for each epoch\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    print(\"\\n Epoch {:} / {:}\".format(epoch + 1, epochs))\n",
    "\n",
    "    # train model\n",
    "    train_loss, _ = train()\n",
    "\n",
    "    # evaluate model\n",
    "    valid_loss, _ = evaluate()\n",
    "\n",
    "    # save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), \"saved_weights.pt\")\n",
    "\n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "\n",
    "    print(f\"\\nTraining Loss: {train_loss:.3f}\")\n",
    "    print(f\"Validation Loss: {valid_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.90      0.60      1430\n",
      "           1       0.74      0.25      0.38      1103\n",
      "           2       0.75      0.24      0.36      1001\n",
      "\n",
      "    accuracy                           0.51      3534\n",
      "   macro avg       0.65      0.46      0.45      3534\n",
      "weighted avg       0.63      0.51      0.46      3534\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1280</td>\n",
       "      <td>82</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>810</td>\n",
       "      <td>280</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>745</td>\n",
       "      <td>16</td>\n",
       "      <td>240</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0     0    1    2\n",
       "row_0                \n",
       "0      1280   82   68\n",
       "1       810  280   13\n",
       "2       745   16  240"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get predictions on test data\n",
    "test_df = pd.read_csv(\"../data/test.csv\", encoding=\"unicode_escape\")\n",
    "test_df = test_df.dropna()\n",
    "test_text, test_label = test_df[\"text\"], test_df[\"sentiment\"]\n",
    "\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    test_text.tolist(),\n",
    "    max_length=max_seq_len,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False,\n",
    ")\n",
    "\n",
    "test_seq = torch.tensor(tokens_test[\"input_ids\"])\n",
    "test_mask = torch.tensor(tokens_test[\"attention_mask\"])\n",
    "test_y = torch.tensor([sentiments[i] for i in test_label.tolist()])\n",
    "\n",
    "# load weights of best model\n",
    "path = \"saved_weights.pt\"\n",
    "model.load_state_dict(torch.load(path))\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds = model(test_seq.to(device), test_mask.to(device))\n",
    "    preds = preds.detach().cpu().numpy()\n",
    "\n",
    "preds = np.argmax(preds, axis=1)\n",
    "print(classification_report(test_y, preds))\n",
    "pd.crosstab(test_y, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
